{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import GPy\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "#from xgboost import plot_importance\n",
    "from numpy import loadtxt\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # ignore DeprecationWarnings from tensorflow\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import gpflow\n",
    "\n",
    "from gpflow.utilities import print_summary, set_trainable\n",
    "from gpflow.ci_utils import ci_niter\n",
    "\n",
    "#from multiclass_classification import plot_posterior_predictions, colors\n",
    "\n",
    "# reproducibility:\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(123)\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.warn(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### try with following versions of input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('X_train_afr_manual_w2v.npy')\n",
    "y_train = np.load('y_train.npy').astype(int)\n",
    "X_test = np.load('X_test_afr_manual_w2v.npy')\n",
    "y_test = np.load('y_test.npy').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train = np.load('X_train_afr_google_w2v.npy')\n",
    "# y_train = np.load('y_train.npy').astype(int)\n",
    "# X_test = np.load('X_test_afr_google_w2v.npy')\n",
    "# y_test = np.load('y_test.npy').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train[:,np.newaxis]\n",
    "y_test=y_test[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Convolutional Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### slecting 244 indiced points \n",
    "X=X_train[::1000]\n",
    "Y=y_train[::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244, 50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.warn(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = [1,50] #[1,50] for our own w2vec model and  [1,300] for  google word2vec model\n",
    "f64 = lambda x: np.array(x, dtype=np.float64)\n",
    "positive_with_min = lambda: tfp.bijectors.AffineScalar(shift=f64(1e-4))(tfp.bijectors.Softplus())\n",
    "constrained = lambda: tfp.bijectors.AffineScalar(shift=f64(1e-4), scale=f64(100.0))(\n",
    "    tfp.bijectors.Sigmoid()\n",
    ")\n",
    "max_abs_1 = lambda: tfp.bijectors.AffineScalar(shift=f64(-2.0), scale=f64(4.0))(\n",
    "    tfp.bijectors.Sigmoid()\n",
    ")\n",
    "\n",
    "patch_shape = [1, 5] \n",
    "### vary patch/filter size and check the final accuracy \n",
    "##(when filter dimensions are changed , accordingly change reshape dimension below)\n",
    "conv_k = gpflow.kernels.Convolutional(gpflow.kernels.SquaredExponential(), IMAGE_SHAPE, patch_shape)\n",
    "conv_k.base_kernel.lengthscales = gpflow.Parameter(1.0, transform=positive_with_min())\n",
    "conv_k.base_kernel.lengthscales = gpflow.Parameter(1.0)\n",
    "#Weight scale and variance are non-identifiable. We also need to prevent variance from shooting off crazily.\n",
    "conv_k.base_kernel.variance = gpflow.Parameter(1.0, transform=constrained())\n",
    "conv_k.weights = gpflow.Parameter(conv_k.weights.numpy(), transform=max_abs_1())\n",
    "conv_f = gpflow.inducing_variables.InducingPatches(\n",
    "    np.unique(conv_k.get_patches(X).numpy().reshape(-1, 5), axis=0)\n",
    ")\n",
    "\n",
    "#### case 1 : substitute X  with inducing points\n",
    "#### case 2 : substitute X with total X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_m = gpflow.models.SVGP(conv_k, gpflow.likelihoods.Bernoulli(), conv_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trainable(conv_m.inducing_variable, False)\n",
    "set_trainable(conv_m.kernel.base_kernel.variance, False)\n",
    "set_trainable(conv_m.kernel.weights, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv elbo before training: -1.8634e+02\n",
      "CPU times: user 34.2 s, sys: 4.87 s, total: 39.1 s\n",
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "conv_training_loss_closure = conv_m.training_loss_closure(data, compile=True)\n",
    "conv_elbo = lambda: -conv_training_loss_closure().numpy()\n",
    "print(\"conv elbo before training: %.4e\" % conv_elbo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Scipy.eval_func.<locals>._tf_eval at 0x7f5c682ad320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Scipy.eval_func.<locals>._tf_eval at 0x7f5c682ad320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "0.013 iter/s\n",
      "CPU times: user 7h 29min 12s, sys: 14min 44s, total: 7h 43min 56s\n",
      "Wall time: 1h 32min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start_time = time.time()\n",
    "res = gpflow.optimizers.Scipy().minimize(\n",
    "    conv_training_loss_closure,\n",
    "    variables=conv_m.trainable_variables,\n",
    "    method=\"l-bfgs-b\",\n",
    "    options={\"disp\": True, \"maxiter\": 100 },\n",
    ")\n",
    "print(f\"{res.nfev / (time.time() - start_time):.3f} iter/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### accuracy of first 100 data points  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 85.65573770491804%\n",
      "Test acc : 82.0%\n",
      "conv elbo after training: -1.0294e+02\n"
     ]
    }
   ],
   "source": [
    "train_acc = np.mean((conv_m.predict_y(X)[0] > 0.5).numpy().astype(\"float\") == Y)\n",
    "test_acc = np.mean((conv_m.predict_y(X_test[0:100])[0] > 0.5).numpy().astype(\"float\") == y_test[0:100])\n",
    "print(f\"Train acc: {train_acc * 100}%\\nTest acc : {test_acc*100}%\")\n",
    "print(\"conv elbo after training: %.4e\" % conv_elbo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Accuracy calculation for entire dataset \n",
    "### as we cant run entire dataset ata a time , calculate acc for 1000 points ata atime and average them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "s1=1\n",
    "s2=1000\n",
    "final_test=0\n",
    "acc_list=[]\n",
    "while s2 < len(X_train):\n",
    "    \n",
    "    test_acc = np.mean((conv_m.predict_y(X_test[s1:s2])[0] > 0.5).numpy().astype(\"float\") == y_test[s1:s2])\n",
    "    #print(test_acc)\n",
    "    #print(f\"Test acc : {test_acc*100}%\")\n",
    "    acc_list.append(test_acc)\n",
    "    #print(acc_list)\n",
    "    s1=s2\n",
    "    s2=s2+1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.8408509189957573\n"
     ]
    }
   ],
   "source": [
    "acc_list_test = np.load('acc_list_test.npy')\n",
    "acc_list_test = acc_list_test[~(np.isnan(acc_list_test))]\n",
    "print(\"test accuracy\",np.mean(acc_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### simialry calculate train accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "s1=1\n",
    "s2=1000\n",
    "final_test=0\n",
    "acc_tr_list=[]\n",
    "while s2 < len(X_train):\n",
    "    \n",
    "    train_acc = np.mean((conv_m.predict_y(X_train[s1:s2])[0] > 0.5).numpy().astype(\"float\") == y_train[s1:s2])\n",
    "    #print(test_acc)\n",
    "    #print(f\"Test acc : {test_acc*100}%\")\n",
    "    acc_tr_list.append(train_acc)\n",
    "    #print(acc_list)\n",
    "    s1=s2\n",
    "    s2=s2+1000\n",
    "#acc=(final_test/s2)*100\n",
    "#print(f\"Test acc : {test_acc*100}%\")\n",
    "#print(\"conv elbo after training: %.4e\" % conv_elbo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"acc_list_train.npy\",acc_tr_list)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.8406165713038141\n"
     ]
    }
   ],
   "source": [
    "acc_list_train = np.load('acc_list_train.npy')\n",
    "acc_list_train = acc_list_train[~(np.isnan(acc_list_train))]\n",
    "print(\"train accuracy\",np.mean(acc_list_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
